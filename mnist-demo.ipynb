{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"simple\" implementation of CNN for MNIST handwriting dataset classification\n",
    "# input: 28px x 28px grayscale image of a handwritten charachter\n",
    "# output: classification label, an integer in [0,9] \n",
    "#\n",
    "# uses data available in tensorflow_datasets (tfds)\n",
    "# stores data as a tensorflow.data.Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_datasets as tfds\n",
    "import IPython.display as display\n",
    "#from PIL import Image                 # NOTE: PIL is not maintained but Pillow is, and this line works with it...\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_OptionsDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>\n",
      "Size of train set: 60000\n",
      "Size of train set: 60000\n",
      "<_OptionsDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>\n",
      "Size of test set: 10000\n",
      "Size of test set: 10000\n"
     ]
    }
   ],
   "source": [
    "# load MNIST data from tensorflow_datasets\n",
    "# data will download from the internet on the first run, retrieved from disk subsequently\n",
    "(ds_train,ds_test), ds_info = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, data_dir='c:\\\\tensorflow-datasets',as_supervised=True,with_info=True)\n",
    "\n",
    "# ensure that data are in correct format\n",
    "assert isinstance(ds_train, tf.data.Dataset)\n",
    "assert isinstance(ds_test, tf.data.Dataset)\n",
    "\n",
    "# display TRAINING dataset details and show number of elements\n",
    "print(ds_train)\n",
    "#print(sum(1 for i in ds_train)) # too slow, plus it \"uses up\" iterator\n",
    "print('Size of train set: ' + np.array2string(tf.data.experimental.cardinality(ds_train).numpy()))\n",
    "print('Size of train set: ' + str(ds_info.splits['train'].num_examples)) # better?\n",
    "\n",
    "# display TRAINING dataset details and show number of elements\n",
    "print(ds_test)\n",
    "#print(sum(1 for i in ds_test)) # too slow, plus it \"uses up\" iterator\n",
    "print('Size of test set: ' + np.array2string(tf.data.experimental.cardinality(ds_test).numpy()))\n",
    "print('Size of test set: ' + str(ds_info.splits['test'].num_examples)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy training dataset so we can look at some samples\n",
    "# if we don't copy it we will \"use up\" the iterator, and it can't just be reset\n",
    "ds_explore = ds_train\n",
    "myit = iter(ds_explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display ONE image from training set\n",
    "# i.e. practice extracting a single image from the iterator and displaying with matplotlib\n",
    "thisInstance = next(myit)\n",
    "fig = plt.imshow(thisInstance[0].numpy().squeeze(),cmap='gray', vmin=0, vmax=255)\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)\n",
    "#img = Image.fromarray(next(myit)[\"image\"])\n",
    "#img.show\n",
    "print('True Label: ' + np.array2string(thisInstance[1].numpy()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract MULTIPLE images from training set\n",
    "fig,sp_axes = plt.subplots(1,5)\n",
    "subplotIdx = 0\n",
    "for inst in ds_explore.take(5):\n",
    "    sp_axes[subplotIdx].imshow(inst[0].numpy().squeeze(),cmap='gray',vmin=0,vmax=255)\n",
    "    sp_axes[subplotIdx].get_xaxis().set_visible(False)\n",
    "    sp_axes[subplotIdx].get_yaxis().set_visible(False)\n",
    "    sp_axes[subplotIdx].set_title( 'True: ' + np.array2string(inst[1].numpy()) )\n",
    "    subplotIdx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, actually build the network\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28,1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten and add dense output layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model, specifying optimization strategy, loss function, and reporting metric(s)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert TRAINING dataset to float datatype, shuffle + batch it, and cache for efficiency\n",
    "# directly from https://www.tensorflow.org/datasets/keras_example\n",
    "def normalize_img(image, label):   \n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)  # data need to be in float32 format, given as uint8\n",
    "ds_train = ds_train.cache()   # cache dataset for more efficient shuffling and prevent reloading in each epoch\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)  # shuffle with buffer set to same size as dataset (ideal)\n",
    "ds_train = ds_train.batch(128)   # batch(N) -> arrange into batches of size N elements each\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE) # performance enhancement https://www.tensorflow.org/guide/data_performance#prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert TESTING dataset to float datatype, batch it, and cache for efficiency\n",
    "# directly from https://www.tensorflow.org/datasets/keras_example\n",
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)   # batch(N) -> arrange into batches of size N elements each\n",
    "ds_test = ds_test.cache()      # cache dataset to prevent reloading in each epoch\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)  # performance enhancement https://www.tensorflow.org/guide/data_performance#prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model in batches and epochs\n",
    "# if not batched this will give ValueError: Input 0 of layer sequential is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [28, 28, 1]\n",
    "history = model.fit(ds_train,epochs=10,validation_data=ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
